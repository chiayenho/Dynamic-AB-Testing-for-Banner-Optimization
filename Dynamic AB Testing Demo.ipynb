{"cells":[{"cell_type":"markdown","source":["# Dynamic A/B Testing Demo\nThis notebook will utilize the sample data files generated by the Data Generator notebook to demonstrate how to perform dynamic A/B testing that continuously monitors the metrics of two strategies and choose the better strategy for next timestep."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"58e19a0d-741a-45cc-89d6-31386a1d5337"}}},{"cell_type":"code","source":["# create port folder on dbfs and initialize timestep 0 by copying from strategy 1\ndbutils.fs.mkdirs('dbfs:/FileStore/port')\ndbutils.fs.cp('dbfs:/FileStore/simulated_data/strategy_0/timestep_0.csv', 'dbfs:/FileStore/port/timestep_0.csv')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"95701bd9-6ff5-453c-98ae-5c6bf88c1a3b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.types import *\n\ndataschema = StructType([ \\\n    StructField(\"banner\",IntegerType(),True), \\\n    StructField(\"product\",StringType(),True), \\\n    StructField(\"time\", TimestampType(), True), \\\n    StructField(\"target\", FloatType(), True)\\\n  ])\n\n# define save_test function to be run each time there is a input\nfrom scipy.stats import ttest_ind_from_stats\ndef save_test(max_timestep):\n    def func(df, timestep):\n        # If the first time step, initialize winner variable\n        if timestep == 0:\n            global winner\n            winner = 0\n            winner_df = spark.createDataFrame([(-1, winner)], schema='timestep: integer, winner: integer')\n            winner_df.write. \\\n            mode(\"append\"). \\\n            saveAsTable(\"ab_test.winner_data\")\n\n        # save input data to hive\n        df = df.withColumn('timestep', F.lit(timestep))\n        df.write. \\\n        mode(\"append\"). \\\n        saveAsTable(\"ab_test.raw_data\")\n\n        # collect statistics for local t-test\n        stats = df.groupby('banner').agg(F.mean('target').alias('target_mean'), \n                                         F.stddev('target').alias('target_std'), \n                                         F.count('target').alias('target_count')).toPandas()\n        stats['target_count'] = stats['target_count'] * 5\n        pvalue = ttest_ind_from_stats(*stats.iloc[:,1:].values.ravel()).pvalue\n\n        # announce new winner\n        if pvalue <= 0.05:\n            winner = int(stats.loc[stats['target_mean'].argmax()]['banner'])\n\n        # save winner to hive\n        winner_df = spark.createDataFrame([(timestep, winner)], \n                                           schema='timestep: integer, winner: integer')\n        winner_df.write. \\\n            mode(\"append\"). \\\n            saveAsTable(\"ab_test.winner_data\")\n        # if timestep haven't reached max, copy next file into port, so that trigger the next streaming task\n        # copying from strategy_0 means we adopt strategy_0 in the next timestep, vice versa.\n        if timestep + 1 <= max_timestep:\n            dbutils.fs.cp('dbfs:/FileStore/simulated_data/strategy_{}/timestep_{}.csv'.format(winner, timestep+1), \n                          'dbfs:/FileStore/port/timestep_{}.csv'.format(timestep+1))\n\n    return func\n\n# initiate the Hive tables for storage\nspark.sql('create database if not exists ab_test')\nspark.sql('drop table if exists ab_test.raw_data')\nspark.sql('drop table if exists ab_test.winner_data')\n\n# Define the streamer. This streamer will monitor the port folder\nstreaming = spark.readStream.schema(dataschema)\\\n    .option(\"maxFilesPerTrigger\", 1)\\\n    .option(\"header\", True)\\\n    .format('csv')\\\n    .load(r\"dbfs:/FileStore/port\")\n# Run the streamer. It will call the save_test function each time a new file appears\nquery = streaming.writeStream.foreachBatch(save_test(max_timestep=120)).start()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"52680f05-99df-45aa-8079-63d1a52bb81b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Stop the streamer when all timesteps are done\nquery.stop()\n# The resulting hive tables are saved as csv in our repository as raw_data.csv and winner_data.csv"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dc855cfd-5a7b-40d7-9d97-1aca15c8f3b6"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"TrendsMkt Demo","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2850781828346817}},"nbformat":4,"nbformat_minor":0}
